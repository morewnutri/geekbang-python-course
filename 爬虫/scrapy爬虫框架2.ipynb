{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'doubanmovie'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-bfee651d8aeb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscrapy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdoubanmovie\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDoubanmovieItem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# from bs4 import BeautifulSoup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscrapy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselector\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSelector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'doubanmovie'"
     ]
    }
   ],
   "source": [
    "import scrapy\n",
    "from doubanmovie.items import DoubanmovieItem\n",
    "# from bs4 import BeautifulSoup\n",
    "from scrapy.selector import Selector\n",
    "\n",
    "\n",
    "class DoubanSpider(scrapy.Spider):\n",
    "    # 定义爬虫名称\n",
    "    name = 'douban'\n",
    "    allowed_domains = ['movie.douban.com']\n",
    "    # 起始URL列表\n",
    "    start_urls = ['https://movie.douban.com/top250']\n",
    "\n",
    "#   注释默认的parse函数\n",
    "#   def parse(self, response):\n",
    "#        pass\n",
    "\n",
    "\n",
    "    # 爬虫启动时，引擎自动调用该方法，并且只会被调用一次，用于生成初始的请求对象（Request）。\n",
    "    # start_requests()方法读取start_urls列表中的URL并生成Request对象，发送给引擎。\n",
    "    # 引擎再指挥其他组件向网站服务器发送请求，下载网页\n",
    "    def start_requests(self):\n",
    "        # for i in range(0, 10):\n",
    "            i=0\n",
    "            url = f'https://movie.douban.com/top250?start={i*25}'\n",
    "            yield scrapy.Request(url=url, callback=self.parse, dont_filter=False)\n",
    "            #dont_filter 设置为 True，是用来解除去重功能。Scrapy 自带 url 去重功能，第二次请求之前会将已发送的请求自动进行过滤处理。所以将 dont_filter 设置为 True 起到的作用是解除去重功能，一旦设置成重 True，将不会去重，直接发送请求。\n",
    "            # url 请求访问的网址\n",
    "            # callback 回调函数，引擎回将下载好的页面(Response对象)发给该方法，执行数据解析\n",
    "            # 这里可以使用callback指定新的函数，不是用parse作为默认的回调参数\n",
    "\n",
    "    # 解析函数\n",
    "    def parse(self, response):\n",
    "        # 打印网页的url\n",
    "        print(response.url)\n",
    "        # 打印网页的内容\n",
    "        # print(response.text)\n",
    "\n",
    "        # soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        # title_list = soup.find_all('div', attrs={'class': 'hd'})\n",
    "        #为什么不直接找title？ --> title可能重复，甚至找到整个网页的标题\n",
    "        movies = Selector(response=response).xpath('//div[@class=\"hd\"]')#//，./这些叫做xpath的路径，\n",
    "        for movie in movies:                        # // -> 从上倒下依次寻找div，匹配条件是div有class属性，并且等于hd\n",
    "        #     title = i.find('a').find('span',).text# / -> 单斜杠表示最上层的div\n",
    "        #     link = i.find('a').get('href')\n",
    "            # 路径使用 / .  .. 不同的含义　\n",
    "            title = movie.xpath('./a/span/text()')# ./ -> 表示从当前的匹配位置继续向下找\n",
    "            #取内容用text（）\n",
    "            link = movie.xpath('./a/@href')       # ../ -> 从当前的上一级的位置继续向下找\n",
    "            #取属性用@href\n",
    "            print('-----------')\n",
    "            print(title)\n",
    "            print(link)\n",
    "            print('-----------')\n",
    "            #提取的数据是列表，以下代码是为了能够释放内容\n",
    "            print(title.extract())#extract --> 如果匹配的有多个数据的话，extract就把所有的数据释放，变成标准的字符串，\n",
    "            print(link.extract())\n",
    "            print(title.extract_first())#extract_first --> 如果有多个元素的话，只释放第一个\n",
    "            print(link.extract_first())\n",
    "            print(title.extract_first().strip())\n",
    "            print(link.extract_first().strip())\n",
    "            #beautifulsoup ->整个页面都找一遍\n",
    "            #xpath -> 用lxml实现的，是用c语言写的，用相对路径来搜索，所以比bieautifulsoup快"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
