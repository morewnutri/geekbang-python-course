{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from bs4 import BeautifulSoup\n",
    "from doubanmovie.items import DoubanmovieItem\n",
    "\n",
    "\n",
    "class DoubanSpider(scrapy.Spider):\n",
    "    # 定义爬虫名称\n",
    "    name = 'douban'\n",
    "    allowed_domains = ['movie.douban.com']\n",
    "    # 起始URL列表\n",
    "    start_urls = ['https://movie.douban.com/top250']\n",
    "\n",
    "#   注释默认的parse函数\n",
    "#   def parse(self, response):\n",
    "#        pass\n",
    "\n",
    "\n",
    "    # 爬虫启动时，引擎自动调用该方法，并且只会被调用一次，用于生成初始的请求对象（Request）。\n",
    "    # start_requests()方法读取start_urls列表中的URL并生成Request对象，发送给引擎。\n",
    "    # 引擎再指挥其他组件向网站服务器发送请求，下载网页\n",
    "    def start_requests(self):\n",
    "        for i in range(0, 10):\n",
    "            url = f'https://movie.douban.com/top250?start={i*25}'\n",
    "            yield scrapy.Request(url=url, callback=self.parse)\n",
    "            # url 请求访问的网址\n",
    "            # callback 回调函数，引擎回将下载好的页面(Response对象)发给该方法，执行数据解析\n",
    "            # 这里可以使用callback指定新的函数，不是用parse作为默认的回调参数\n",
    "\n",
    "    # 解析函数\n",
    "    def parse(self, response):\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        title_list = soup.find_all('div', attrs={'class': 'hd'})\n",
    "        # 在Python中应该这样写\n",
    "        for i in title_list:#for i in range(len(title_list)):\n",
    "            # 在items.py定义\n",
    "            item = DoubanmovieItem()\n",
    "            title = i.find('a').find('span').text\n",
    "            link = i.find('a').get('href')\n",
    "            item['title'] = title\n",
    "            item['link'] = link\n",
    "            yield scrapy.Request(url=link, meta={'item': item}, callback=self.parse2)\n",
    "\n",
    "    # 解析具体页面\n",
    "    def parse2(self, response):\n",
    "        item = response.meta['item']\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        content = soup.find('div', attrs={'class': 'related-info'}).get_text().strip()#从网页源代码中找到简介相对应的标签\n",
    "        item['content'] = content #item这些都是终端创建startproject时创建的                                             #strip（）可以特殊符号两端去掉\n",
    "        yield item \n",
    "        \n",
    "        \n",
    "        #通过终端生成的文件包括setting.py,item.py,pipline.py,int.py等。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
